# Exercise 4: Scaling techniques

## Introduction

The hands-on exercise for this week focuses on: 1) scaling texts ; 2) implementing scaling techniques using `quanteda`. 

In this tutorial, you will learn how to:
  
* Scale texts using the "wordfish" algorithm
* Scale texts gathered from online sources
* Replicate analyses by @kaneko_estimating_2021

Before proceeding, we'll load the packages we will need for this tutorial.

```{r, echo=F}
library(kableExtra)
```

```{r, message=F}
library(dplyr)
library(quanteda) # includes functions to implement Lexicoder
library(quanteda.textmodels) # for estimating similarity and complexity measures
library(quanteda.textplots) #for visualizing text modelling results
```

In this exercise we'll be using the dataset we used for the sentiment analysis exercise. The data were collected from the Twitter accounts of the top eight newspapers in the UK by circulation. The tweets include any tweets by the news outlet from their main account. 

## Importing data

We can download the dataset with:

##```{r} tweets <- readRDS("data/sentanalysis/newstweets.rds")

If you're working on this document from your own computer ("locally") you can download the tweets data in the following way:

```{r}
tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/sentanalysis/newstweets.rds?raw=true")))

```

We first take a sample from these data to speed up the runtime of some of the analyses. 

```{r}

tweets <- tweets %>%
  sample_n(20000) ##select random n rows from a dataframe

```

## Construct `dfm` object

Then, as in the previous exercise, we create a corpus object, specify the document-level variables by which we want to group, and generate our document feature matrix. 

```{r}

#make corpus object, specifying tweet as text field
tweets_corpus <- corpus(tweets, text_field = "text")

#add in username document-level information
docvars(tweets_corpus, "newspaper") <- tweets$user_username ##docvars is used to access document variables in the corpus. here, we want to access the usernames in the tweets_corpus corpus

dfm_tweets <- dfm(tokens(tweets_corpus),
                  remove_punct = TRUE, 
                  remove = stopwords("english")) ##dfm() creates document-feature matrix to format text data in suitable format for analysis
##tokens() splits each tweet into individual words

```


##dfm_tweets <- readRDS("data/wordscaling/dfm_tweets.rds")


We can then have a look at the number of documents (tweets) we have per newspaper Twitter account. 

```{r}

## number of tweets per newspaper
table(docvars(dfm_tweets, "newspaper"))

```

And this is what our document feature matrix looks like, where each word has a count for each of our eight newspapers. 

```{r}

dfm_tweets ##to view the matrix

```

## Estimate wordfish model

Once we have our data in this format, we are able to group and trim the document feature matrix before estimating the wordfish model.

```{r}
# compress the document-feature matrix at the newspaper level
dfm_newstweets <- dfm_group(dfm_tweets, groups = newspaper) ##dfm_group() function used to group the dfm. in this code the dfm is grouped by newpaper

# remove words not used by two or more newspapers
dfm_newstweets <- dfm_trim(dfm_newstweets, 
                                min_docfreq = 2, docfreq_type = "count")
##dfm_trim() used to remove words from the dfm
##min_docfreq = 2 means that terms that appear in two or more documents will be shown
##docfreq_type = "count" shows that the document frequency is based on the number of documents


## size of the document-feature matrix
dim(dfm_newstweets)
## shows the number of rows (documents) and columns (features/words) of the dfm

#### estimate the Wordfish model ####
##wordfish model is a method to analyse text data, primarily designed to estimate ideological or thematic positions of words and documents
set.seed(123L) ##this function. creates a random number generator with a specific value that repeat each time
##putting a value in set.seed() allows you to get the same sequence of numbers.
dfm_newstweets_results <- textmodel_wordfish(dfm_newstweets, 
                                             sparse = TRUE) ##sparse indicates that there are a lot of 0s in the dfm
##textmodel_wordfish() function for wordfish, applying the model to dfm_newtweets

```

And this is what results.

```{r}
summary(dfm_newstweets_results)
```

We can then plot our estimates of the $\theta$s---i.e., the estimates of the latent newspaper position---as so.

```{r}
textplot_scale1d(dfm_newstweets_results)

##textplot_scale1d() used to visulise scale of documents and words typically for wordfish model. this reveals patterns or relationship of words based on position in the dimensions
```

Interestingly, we seem not to have captured ideology but some other tonal dimension. We see that the tabloid newspapers are scored similarly, and grouped toward the right hand side of this latent dimension; whereas the broadsheet newspapers have an estimated theta further to the left.

Plotting the "features," i.e., the word-level betas shows how words are positioned along this dimension, and which words help discriminate between news outlets.

```{r}

textplot_scale1d(dfm_newstweets_results, margin = "features")
##in this visualisation, the focus of scaling is through features (words). margin = "features" allows this.

```

And we can also look at these features.

```{r}

features <- dfm_newstweets_results[["features"]]
##this creates a new variable called features, by extracting all the words (removing newspaper column) from dfm_newstweets_results

betas <- dfm_newstweets_results[["beta"]]
##this creates a new variable called beta, extracting all the scaling position of the words (removes estimated psi and keeps beta).

feat_betas <- as.data.frame(cbind(features, betas))
feat_betas$betas <- as.numeric(feat_betas$betas)
##this creates a table with the features and betas values 
##cbind() combines the features and beta values, converts the previous matrix into a table.
## as.numeric() converts the betas column to numeric data type

feat_betas %>%
  arrange(desc(betas)) %>%
  top_n(20) %>% 
  kbl() %>%
  kable_styling(bootstrap_options = "striped")

##top() selects how many rows to show
##kbl() shows the data as a table
##kable_styling(bootstrap_options = "striped") applies striped rows in the table for better readability

```

These words do seem to belong to more tabloid-style reportage, and include emojis relating to film, sports reporting on "cristiano" as well as more colloquial terms like "saucy."

## Replicating Kaneko et al.

This section adapts code from the replication data provided for @kaneko_estimating_2021 [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/EL3KYD). We can access data from the first study by @kaneko_estimating_2021 in the following way. 



##kaneko_dfm <- readRDS("data/wordscaling/study1_kaneko.rds")



If you're working locally, you can download the `dfm` data with:

```{r}
kaneko_dfm  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/wordscaling/study1_kaneko.rds?raw=true")))

```

This data is in the form a document-feature-matrix. We can first manipulate it in the same way as @kaneko_estimating_2021 by grouping at the level of newspaper and removing infrequent words.

```{r}
table(docvars(kaneko_dfm, "Newspaper"))
## prepare the newspaper-level document-feature matrix
# compress the document-feature matrix at the newspaper level
kaneko_dfm_study1 <- dfm_group(kaneko_dfm, groups = Newspaper)
# remove words not used by two or more newspapers
kaneko_dfm_study1 <- dfm_trim(kaneko_dfm_study1, min_docfreq = 2, docfreq_type = "count")

## size of the document-feature matrix
dim(kaneko_dfm_study1)

```

## Exercises

1. Estimate a wordfish model for the @kaneko_estimating_2021 data

```{r}


#### estimate the Wordfish model ####
##wordfish model is a method to analyse text data, primarily designed to estimate ideological or thematic positions of words and documents
set.seed(123L) ##this function. creates a random number generator with a specific value. 
##putting a value in set.seed() allows you to get the same sequence of numbers.
kaneko_dfm_study1_results <- textmodel_wordfish(kaneko_dfm_study1, 
                                             sparse = TRUE)
##textmodel_wordfish() function for wordfish, applying the model to kaneko_dfm_study1
```

2. Visualize the results

```{r}
##showing the results of the wordfish analysis
summary(kaneko_dfm_study1_results)
```
```{r}
#plotting the estimated positions of the newspapers
textplot_scale1d(kaneko_dfm_study1_results)

##textplot_scale1d() used to visualise scale of documents and words typically from wordfish method. this reveals patterns or relationship of words based on position in the dimensions 
```
```{r}
#plotting estimated positions of words in the wordfish dimension
textplot_scale1d(kaneko_dfm_study1_results, margin = "features")
##in this visualisation, the focus of scaling is through features (words). margin = "features" allows this.
```

```{r}
#closer look at the features results

features <- kaneko_dfm_study1_results[["features"]]

betas <- kaneko_dfm_study1_results[["beta"]]


feat_betas <- as.data.frame(cbind(features, betas))
feat_betas$betas <- as.numeric(feat_betas$betas)

feat_betas %>%
  arrange(desc(betas)) %>%
  top_n(20) %>% 
  kbl() %>%
  kable_styling(bootstrap_options = "striped")

##top() selects how many rows to show
##kbl() shows the data as a table
##kable_styling(bootstrap_options = "striped") applies striped rows in the table for better readability

```





